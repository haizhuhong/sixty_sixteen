{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git@github.com:pointOfive/sixty_sixteen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) https://brew.sh\n",
    "# `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"`\n",
    "# (2) https://git-scm.com/download/mac\n",
    "# `brew install git`\n",
    "# https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh\n",
    "# `ssh-keygen -t rsa -b 4096 -C \"iam.scottschwartz@gmail.com\"`\n",
    "# `ls -al ~/.ssh`\n",
    "# pbcopy < ~/.ssh/id_rsa.pub\n",
    "# <github/settings/SSH and GPG Keys/New SSH key><ctrl-v: paste key><name it><save>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "appnope                   0.1.0           py38h32f6830_1001    conda-forge\r\n",
      "argon2-cffi               20.1.0           py38h4d0b108_1    conda-forge\r\n",
      "async_generator           1.10                       py_0    conda-forge\r\n",
      "attrs                     20.2.0             pyh9f0ad1d_0    conda-forge\r\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\n",
      "backports                 1.0                        py_2    conda-forge\r\n",
      "backports.functools_lru_cache 1.6.1                      py_0    conda-forge\r\n",
      "blas                      1.0                         mkl  \r\n",
      "bleach                    3.2.1              pyh9f0ad1d_0    conda-forge\r\n",
      "brotlipy                  0.7.0           py38h4d0b108_1000    conda-forge\r\n",
      "ca-certificates           2020.6.20            hecda079_0    conda-forge\r\n",
      "certifi                   2020.6.20        py38h32f6830_0    conda-forge\r\n",
      "cffi                      1.14.3           py38hed5b41f_0  \r\n",
      "chardet                   3.0.4           py38h32f6830_1007    conda-forge\r\n",
      "cryptography              3.1.1            py38h52adbb4_0    conda-forge\r\n",
      "cycler                    0.10.0                   py38_0  \r\n",
      "decorator                 4.4.2                      py_0    conda-forge\r\n",
      "defusedxml                0.6.0                      py_0    conda-forge\r\n",
      "entrypoints               0.3             py38h32f6830_1001    conda-forge\r\n",
      "freetype                  2.10.2               ha233b18_0  \r\n",
      "idna                      2.10               pyh9f0ad1d_0    conda-forge\r\n",
      "importlib-metadata        2.0.0            py38h32f6830_0    conda-forge\r\n",
      "importlib_metadata        2.0.0                         0    conda-forge\r\n",
      "intel-openmp              2019.4                      233  \r\n",
      "ipykernel                 5.3.4            py38h23f93f0_0    conda-forge\r\n",
      "ipython                   7.18.1           py38h1cdfbd6_0    conda-forge\r\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\r\n",
      "jedi                      0.15.2                   py38_0    conda-forge\r\n",
      "jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\r\n",
      "jpeg                      9b                   he5867d9_2  \r\n",
      "json5                     0.9.5              pyh9f0ad1d_0    conda-forge\r\n",
      "jsonschema                3.2.0            py38h32f6830_1    conda-forge\r\n",
      "jupyter_client            6.1.7                      py_0    conda-forge\r\n",
      "jupyter_core              4.6.3            py38h32f6830_1    conda-forge\r\n",
      "jupyterlab                2.2.8                      py_0    conda-forge\r\n",
      "jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge\r\n",
      "jupyterlab_server         1.2.0                      py_0    conda-forge\r\n",
      "kiwisolver                1.2.0            py38h04f5b5a_0  \r\n",
      "lcms2                     2.11                 h92f6f08_0  \r\n",
      "libcxx                    10.0.0                        1  \r\n",
      "libedit                   3.1.20191231         h1de35cc_1  \r\n",
      "libffi                    3.3                  hb1e8313_2  \r\n",
      "libgfortran               3.0.1                h93005f0_2  \r\n",
      "libpng                    1.6.37               ha441bb4_0  \r\n",
      "libsodium                 1.0.18               haf1e3a3_1    conda-forge\r\n",
      "libtiff                   4.1.0                hcb84e12_1  \r\n",
      "lz4-c                     1.9.2                hb1e8313_1  \r\n",
      "markupsafe                1.1.1            py38h4d0b108_1    conda-forge\r\n",
      "matplotlib                3.3.1                         0  \r\n",
      "matplotlib-base           3.3.1            py38h181983e_0  \r\n",
      "mistune                   0.8.4           py38h64e0658_1001    conda-forge\r\n",
      "mkl                       2019.4                      233  \r\n",
      "mkl-service               2.3.0            py38hfbe908c_0  \r\n",
      "mkl_fft                   1.2.0            py38hc64f4ea_0  \r\n",
      "mkl_random                1.1.1            py38h959d312_0  \r\n",
      "nbclient                  0.5.0                      py_0    conda-forge\r\n",
      "nbconvert                 6.0.6            py38h32f6830_0    conda-forge\r\n",
      "nbformat                  5.0.7                      py_0    conda-forge\r\n",
      "ncurses                   6.2                  h0a44026_1  \r\n",
      "nest-asyncio              1.4.1                      py_0    conda-forge\r\n",
      "notebook                  6.1.4            py38h32f6830_0    conda-forge\r\n",
      "numpy                     1.19.1           py38h3b9f5b6_0  \r\n",
      "numpy-base                1.19.1           py38hcfb5961_0  \r\n",
      "olefile                   0.46                       py_0  \r\n",
      "openssl                   1.1.1h               haf1e3a3_0    conda-forge\r\n",
      "packaging                 20.4               pyh9f0ad1d_0    conda-forge\r\n",
      "pandas                    1.1.2            py38hb1e8313_0  \r\n",
      "pandoc                    2.10.1               haf1e3a3_0    conda-forge\r\n",
      "pandocfilters             1.4.2                      py_1    conda-forge\r\n",
      "parso                     0.5.2                      py_0  \r\n",
      "pexpect                   4.8.0            py38h32f6830_1    conda-forge\r\n",
      "pickleshare               0.7.5           py38h32f6830_1001    conda-forge\r\n",
      "pillow                    7.2.0            py38ha54b6ba_0  \r\n",
      "pip                       20.2.3                   py38_0  \r\n",
      "prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge\r\n",
      "prompt-toolkit            3.0.7                      py_0    conda-forge\r\n",
      "ptyprocess                0.6.0                   py_1001    conda-forge\r\n",
      "pycparser                 2.20               pyh9f0ad1d_2    conda-forge\r\n",
      "pygments                  2.7.1                      py_0    conda-forge\r\n",
      "pyopenssl                 19.1.0                     py_1    conda-forge\r\n",
      "pyparsing                 2.4.7                      py_0  \r\n",
      "pyrsistent                0.17.3           py38h4d0b108_0    conda-forge\r\n",
      "pysocks                   1.7.1            py38h32f6830_1    conda-forge\r\n",
      "python                    3.8.5                h26836e1_1  \r\n",
      "python-dateutil           2.8.1                      py_0  \r\n",
      "python_abi                3.8                      1_cp38    conda-forge\r\n",
      "pytz                      2020.1                     py_0  \r\n",
      "pyzmq                     19.0.2           py38hb1e8313_1  \r\n",
      "readline                  8.0                  h1de35cc_0  \r\n",
      "requests                  2.24.0             pyh9f0ad1d_0    conda-forge\r\n",
      "scipy                     1.5.0            py38hbab996c_0  \r\n",
      "send2trash                1.5.0                      py_0    conda-forge\r\n",
      "setuptools                49.6.0                   py38_1  \r\n",
      "six                       1.15.0                     py_0  \r\n",
      "sqlite                    3.33.0               hffcf06c_0  \r\n",
      "terminado                 0.9.1            py38h32f6830_0    conda-forge\r\n",
      "testpath                  0.4.4                      py_0    conda-forge\r\n",
      "tk                        8.6.10               hb0a8c7a_0  \r\n",
      "tornado                   6.0.4            py38h1de35cc_1  \r\n",
      "traitlets                 5.0.4                      py_1    conda-forge\r\n",
      "urllib3                   1.25.10                    py_0    conda-forge\r\n",
      "wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\r\n",
      "webencodings              0.5.1                      py_1    conda-forge\r\n",
      "wheel                     0.35.1                     py_0  \r\n",
      "xz                        5.2.5                h1de35cc_0  \r\n",
      "zeromq                    4.3.2                hb1e8313_3  \r\n",
      "zipp                      3.2.0                      py_0    conda-forge\r\n",
      "zlib                      1.2.11               h1de35cc_3  \r\n",
      "zstd                      1.4.5                h41d2c2f_0  \r\n"
     ]
    }
   ],
   "source": [
    "! conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl (165.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.2 MB 577 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 587 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 492 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 658 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 681 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 579 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1 MB 680 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 698 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.13.0-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 589 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 574 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 593 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 575 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.6.0.post20200925)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 536 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.2-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 698 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 468 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 514 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 661 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gck8gd/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 558 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 572 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ba21ae0d5c8dcaace8461b2b64093b48a9a51415662a97025afdafab77abde39\n",
      "  Stored in directory: /Users/gck8gd/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32641 sha256=7cb82a71fbd1bef8da6724e3da1590c8ba7de4df4994a6f384fcf8a5ca14986f\n",
      "  Stored in directory: /Users/gck8gd/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: gast, grpcio, google-pasta, numpy, keras-preprocessing, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, tensorboard-plugin-wit, absl-py, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, markdown, werkzeug, tensorboard, astunparse, opt-einsum, termcolor, h5py, wrapt, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/install\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/getting_started/intro_to_keras_for_engineers/\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = stats.uniform(0,10).rvs(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30798555, -0.32449238, -0.96098832])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://mc-stan.org/docs/2_24/functions-reference/lkj-correlation.html\n",
    "m = [1,0,0]\n",
    "\n",
    "def mvn(m):\n",
    "#    cov = np.diag(3*[np.max(m)]) + np.max(m)*np.ones((3,3))\n",
    "#    return cov #, cov=cov\n",
    "    return stats.multivariate_normal(mean=m, cov=np.diag(3*[np.max(m)]) + np.max(m)*np.ones((3,3))).rvs(1)\n",
    "    \n",
    "mvn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvn = np.vectorize(mvn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9 into shape (1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-b9f45cd0009e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmvn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2121\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m             \u001b[0;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-26bfea13f3f7>\u001b[0m in \u001b[0;36mmvn\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    cov = np.diag(3*[np.max(m)]) + np.max(m)*np.ones((3,3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#    return cov #, cov=cov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmvn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m--> 365\u001b[0;31m         return multivariate_normal_frozen(mean, cov,\n\u001b[0m\u001b[1;32m    366\u001b[0m                                           \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                                           seed=seed)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \"\"\"\n\u001b[1;32m    740\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultivariate_normal_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         self.dim, self.mean, self.cov = self._dist._process_parameters(\n\u001b[0m\u001b[1;32m    742\u001b[0m                                                             None, mean, cov)\n\u001b[1;32m    743\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning_6018/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m_process_parameters\u001b[0;34m(self, dim, mean, cov)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mcov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 9 into shape (1,1)"
     ]
    }
   ],
   "source": [
    "\n",
    "mvn([[1,0,0],[0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 0], 0, 0], [0, [1, 0], 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m,0,0],[0,m,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None, None, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Center-crop images to 150x150\n",
    "x = CenterCrop(height=150, width=150)(inputs)\n",
    "# Rescale images to [0, 1]\n",
    "x = Rescaling(scale=1.0 / 255)(x)\n",
    "\n",
    "# Apply some convolution and pooling layers\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some Pythoning that we could do\n",
    "0. affine function\n",
    "1. little maths: maxout, sigmoid, softplus (not hyperbolic tangent)\n",
    "2. mixture specifier\n",
    "\n",
    "or, exceptiallence actuallutely\n",
    "\n",
    "## Mixture density networks\n",
    "- this would be a good coding example? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Output units as parameters for distributions\n",
    "\n",
    "- parameters of distributions (rather than distributions): $p(y|f(\\mathbf{x};\\boldsymbol{\\theta}))$    \n",
    "    - gradients of log, multiplication, addition are well-behaved.\n",
    "    - heterskedastic model: $\\sqrt{\\frac{f_\\beta(x)}{2\\pi}} e^{-\\frac{f_\\beta(x)}{2}(y-f_\\mu(x))^2}$ or $\\frac{|f_\\beta(x)|^{1/2}}{\\sqrt{2\\pi}^k} e^{-\\frac{1}{2}(y-f_\\mu(x))^T f_\\beta(x)(y-f_\\mu(x)) }$  \n",
    "        - determinant of diagonal matrix $f_\\beta(x)$ is product of diagonals\n",
    "        - if you take the log of this the derivative is $log(f_\\beta(x)_i) + (y-f_\\mu(x))_if_\\beta(x)_i(y-f_\\mu(x))_i$\n",
    "            - log, addition, multiplication...\n",
    "        - wouldn't want to parameterize by varaince (or you'd be dividing)\n",
    "        - standard deviation would also be a bad choice... the gradient near 0 can vanish in squaring (making it difficult to learn parameters that are squared). \n",
    "    - regardless... covariance matrix must stay PD: eigenvalues of preicison matrix are inverse of those of covariance matrix... so eigenvalues of preicison matrix must be positive.\n",
    "    - can use softplus to active affine function in positives only\n",
    "    - for a full covariance matrix $\\Sigma(x) = B(x)B(x)^T$ and $B(x)$ may be unconstrained.\n",
    "        - likelihood calculation here is is $O(n^3)$ -- for determinant and inverse of $\\Sigma$, or it's eigen decomposition (or that of $B(x)$)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes I made/took to create my questions\n",
    "\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- The biggest choices in network architecture involve *depth* and *width*\n",
    "    - Parameters here are called weights ($W$) and biases ($c$): $g(\\mathbf{W}^Tx+\\mathbf{c})$\n",
    "        - You'll most likely be using *ReLU*: $max(0, \\mathbf{W}^Tx+\\mathbf{c})$ \n",
    "            - The caption in Figure 6.3 is very interesting and definitely worth reading!\n",
    "- Nonlinear layer outputs (i.e., activation functions, $g$ above) chained together allow networks to\n",
    "    - NOT require manual feature engineering $x_1^2$, $x_1\\cdot x_2$, etc.\n",
    "    - NOT rely on predefined generic transformations $\\phi$ into high dimensional spaces\n",
    "        - while these can capture many nonlinearities, they also *very* likely produce a local smootheness behavior...\n",
    "    - LEARN transformations $\\hat \\phi$: $\\hat \\phi(x; \\boldsymbol{\\theta})^T\\mathbf{w}$\n",
    "        - that can \"do feature engineering for you\" with an extremely generic transformation capability if we consider a broad class of $\\phi(x; \\boldsymbol{\\theta})$       \n",
    "        - and only cost \"giving up\" on optimizing a convex cost function\n",
    "        - and in choosing the character of the class of $\\phi(x; \\boldsymbol{\\theta})$, the \"manual expertise\" of the analyst can still influence the score of transformation considered (by just defining the general functional forms, rather than the specific forms used, which is even better actually)\n",
    "        \n",
    "## Optimizing FF Networks\n",
    "\n",
    "- Nonlinearity (usually) means loss of convexity \n",
    "    - so linear solvers and convex optimization procedures can't generally be used for FF networks\n",
    "- Fitting by the gradient, though, is a universally applicable optimization methodology\n",
    "    - Other ML methodologies could use it but they usually use shortcuts instead\n",
    "    - but, when the data is large and those shortcuts are not longer computationally tractable, they'll also just use SGD as the universal optimization method it is\n",
    "- When optimizing non-convex cost functions, there are no \"optimum guarantees\" from SGD\n",
    "    - The gradient is followed: and hopefully it this can drive down the cost to a sufficiently low enough value\n",
    "        - cross-entropy (negative log likelihood of natural probability distribution) is the usual choice for the cost function\n",
    "            - this is a result of the principle of maximum likelihood which suggests we use $−\\log p(y | x; \\theta)$ as our cost function: more on this below...        \n",
    "            - what is needed from a cost function, though, is that the gradient be a good guide: i.e., if it's pretty flat (near 0) then it doesn't help us...\n",
    "             - negative log likelihood often protects against this happening (it's a log which counteracts exponentiation that happens, e.g., in softmax, which causes saturation)\n",
    "                 - you do need to be careful, though, that the log likelihood for many models can become arbitrarily good (larger and larger magnitude negative values)\n",
    "                     - logistic regression doesn't work like this since 0 is the minimum log likelihood value achievable for this model         \n",
    "    - Initial values matter: FF networks should initialize weights to small values, biases should be small positive initializations, or 0\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "## Output units\n",
    "\n",
    "- *linear* units (*affine functions*) don't pose much challenge for gradient based optimization since they don't saturate\n",
    "- *logistic sigmoid* function (*inverse logit function*) on *affine functions* (which then define the *logit function*)\n",
    "    - are generally created by normalizinig *exponentiated affine function*\n",
    "    - but since $\\frac{C \\exp(z_0)}{C \\exp(z_0) + C \\exp(z_1)}$ is constant regardless of $C$, $z_0$ is fixed to $0$ and only $z_1$ is used (or only $z_1$ and $z_2$ in the case of three classes, and so on...)\n",
    "    - For two classes, then, the normalized sigmoid can be expressed as: $\\sigma((2y-1)z) = \\frac{1}{1+e^{-(2y-1)z}} = \\frac{1}{1+e^{-2yz}e^z} = \\frac{e^{yz}}{e^{yz}+e^{-yz}e^z} = \\frac{e^{yz}}{e^{yz}+e^{(1-y)z}}$\n",
    "    - and since this defines the likelihood of a Bernoulli distribution, the cross-entropy (negative log likelihood) is: $-log\\left( \\frac{1}{1+e^{-(2y-1)z}} \\right) = log \\left(1+e^{-(2y-1)z} \\right) = log \\left(1+e^{(1-2y)z} \\right) = \\zeta((1-2y)z) \\underset{y=0,z>0}{\\overset{y=1, z<0}{\\approx}} |z|$ (whenever $z$ is wrong)\n",
    "        - which is easily differentiated with respect to $z$, and so, again as with identity units, does not pose much challenge for gradient based optimization since it doesn't saturate (and remains informative) when $z$ is wrong\n",
    "    - On the other hand, the loss function $(y- \\hat p)^2$ gets flatter and flatter the closer $\\hat p$ is to $1-y$ (i.e., the more wrong $\\hat p$ is)\n",
    "        - thus we always use cross-entropy (as opposed to, e.g., MSE) for sigmoid outputs\n",
    "    - Also, note that computing should be done with z where possible, as opposed to $\\sigma((2y-1)z)$ which can have under/overflow problems.\n",
    "    \n",
    "- *softmax* generalizes the *logistic sigmoid* function to more than two classes\n",
    "    - It provides a continuous and differentiable, i.e., \"soft\" version, of argmax\n",
    "    - Since $\\log \\text{softmax}(z_i) = z_i - \\log \\sum_j \\exp(z_j) \\approx z_i - \\underset{j}{\\max} z_j$\n",
    "        - if biggest $z_i$ is correct, then these will mostly cancel, and the likelihood will be driven by other incorrect observations\n",
    "            - i.e., $z_i$ doesn't matter but $\\underset{j\\not=i}{\\max} z_j$ hurts you so it will try to be small\n",
    "    - $\\log$ cost functions are needed to \"reverse the saturation\" of softmax under exponentiation\n",
    "    - $\\text{softmax}(\\mathbf{z}) = \\text{softmax}(\\mathbf{z} - \\underset{j}{\\max} z_j)$ is numerically stable\n",
    "    - If you don't set $z_0=0$ you are technically \"overparameterized\"; but, this doesn't really seem to matter in practice [unlike indicator varables where it does for multicollinearity in linear modesl (though not for trees)].\n",
    "\n",
    "## Hidden units\n",
    "\n",
    "### ReLU\n",
    "\n",
    "- Often activation functions have undefined derivatives at certain location (e.g., ReLU)\n",
    "    - in practice this works fine computationally\n",
    "        - because we're generally following derivatives to reach a minimum (gradient $\\mathbf{0}$) but never actually get there\n",
    "    - so if the derivatives is not defined at those minimums, it does matter -- they're never actually evaluated\n",
    "    - not to mention numerical representation is inexact, and left and right derivatives are numerically what's used anyway (and those exist)\n",
    "- While activation functions are currently an active area of research, ReLU $\\max\\{0,z\\}$ is an *excellent* default choice these days\n",
    "    - ReLU has no second order derivative (i.e., that's 0), and derivative is 1 when it's activated \n",
    "        - so ReLU provide a super easy and predictable gradient surface) \n",
    "        - and *most IMPORTANTLY* have a ZERO derivative degregation from saturdation when activated\n",
    "        - the basic story is models with behavior that's more \"linear\" are easier to fit with gradient descent.\n",
    "            -  Internal affine weights are generally initialize to a small positive value (like 0.1) so the activation function starts \"on\" for most observations (and the derivative exists)\n",
    "\n",
    "- There are some generalized versions of ReLU \n",
    "- Performance of choices beyond ReLU is very hard to predict actual performance \n",
    "    - it's a very \"trial and error\" process\n",
    "    - and often they just work about the same, only occasionally working better than ReLU...\n",
    "- These generalizations exist because ReLU is flat -- i.e., can't learn -- when it's inactive -- and the generalizations try to address this\n",
    "\n",
    "    - $\\max\\{0,z_i\\} + \\alpha_i \\min\\{0,z_i\\}$\n",
    "        - *absolute value rectification*: $\\alpha_i=-1$ give $|z|$\n",
    "        - *leaky ReLU*: $\\alpha_i \\approx 0.01$\n",
    "        - *Parametric ReLU (PReLU)*: makes $\\alpha_i$ learnable\n",
    "        - *maxout* units: $f(z)_i = \\underset{z_j \\in \\mathbf{z}[(1+gk−k):gk]}{\\max} z_j(=(\\mathbf{W}^Tx+\\mathbf{b})_j)$\n",
    "            - $k$ $z_j$ are considered for $f(z)_i$ (one for each $z_j \\in \\mathbf{z}[(1+gk−k):gk]$), and each $z_j$ is a linear function in $x$ (note that $x$ is a scalar)\n",
    "                - plotting each of these $k$ $z_j$ versus each $x$ (again, $x$ is just a scalar  here) is a bunch of lines over $x$\n",
    "                - taking the the supremum of these $k$ linear functions makes a piecewise linear function (with $k$ pieces, and which must be convex)\n",
    "                - since different lines are defined by weights, and the weights are learned, a convex activation function can be learned... larger $k$ means a more flexible piecewise linear function.\n",
    "            - often more regularization is required to fit maxout activation functions since the activation functions have $2k$ parameters (defined by the weights and biases) (or $k$ parameter vectors when x is not a scalar and the weights become vectors)\n",
    "            - and since each $z_j$ is considered in multiple $f(z)_i$, there is some redundancy that projects against (*catastropic forgettting*).\n",
    "            - there are some ways to reduce the number of necessary weights in maxout activation functions\n",
    "\n",
    "### Others\n",
    "\n",
    "- Logistic Sigmoid $\\sigma(z) = \\frac{1}{1+e^{-z}}$ and Hyperbolic Tangent $tanh(z) = 2\\sigma(2z) - 1$\n",
    "    - saturate everywhere except near 0\n",
    "    - work as an output layer if there's an appropriate cost function to \"reverse\" the saturation.\n",
    "    - $tanh(z)$ resembles liner around 0 (unlike $\\sigma(z)$ which is not symmetric around 0) if activations are kept small\n",
    "        - this linearity can be helpful for gradient calculations \n",
    "- Others\n",
    "    - *identity* can be used to do \"factor models\" first and feeding factors into a network as opposed to all the data\n",
    "    - *softmax* can provide an internal discrete probability distribution\n",
    "    - *RBF* provices template matching\n",
    "        - but RBF saturates for most $x$... so is hard to fit/optimize good models (i.e., calculate gradients) with RBF...\n",
    "    - *softplus* does not work as well as ReLU in practice... strangely/counter intuititively...\n",
    "    - *hard tanh* $\\max(−1, min(1, z))$ just looks like a \"ReLU'd\" version of $tanh$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## Arichtecture\n",
    "\n",
    "- A singly hidden layer networks can fit data well\n",
    "- but deeper (chained) networks (with overall less parameters) tend to provide better generalization(!)\n",
    "    - Deeper networks encode a prior belief in *composition* of features as relevant, i.e., features of varaition are in turn composed of simpler features of variation\n",
    "        - and this empirically does seem to have improved generalization properties\n",
    "- The *universal approximation theorem* says: networks with a \"squashing\" layer (and this result  shown for ReLU as well) and enough hidden units *can* represent any function (and match it's derivatives)\n",
    "    - although *actually doing this* is another thing\n",
    "        - *making* the network give a good approximation/representation of a function means actually getting SGD to find \"good enough\" network weights (in the context of the given architecture and it's activation functions) while not ending up overfitting...\n",
    "    - And remember: the *no free lunch* theorem says there's no best algorithm to fit the MLP which will generalize the best universally \n",
    "    - And also *notice* that: depending on how complex the function it the number of required hidden units this *\"can do it\"* can actually become intractible...\n",
    "    \n",
    "                \n",
    "## Back propegation \n",
    "- FF networks forward propegate data to a cost function\n",
    "- Effects of changes to the parameters (gradients) to how incoming x effects the cost function can be found by going the opposite way with \"backprop\"\n",
    "- *Backprop* just computes gradients of parameters with respect to the cost function\n",
    "- Following the gradient (e.g., by SGD) is the *optimzation* procedure\n",
    "- Gradients are computed using the *chain rule* $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x}$\n",
    "    - 6.45/6 show vector generalizations\n",
    "    - 6.47 shows shows tensor generalizations\n",
    "\n",
    "## Symbolic Representation of Computational Graphs\n",
    "- Computational graphs can be formally defined as in figure 6.8\n",
    "    - these are *symbolic representation*\n",
    "    - Torch/Caffe take a symbolic representations and inputs and produce *symbol-to-number diﬀerentiation*, i.e., the derivatives for these numbers (extended graph is just never exposed)\n",
    "    - Theano/Tensorflow add nodes to the computational graph which will produce the gradients (e.g., 6.10)\n",
    "        - higher order derivativs can be calculated as derivatives on the graph with derivatives    \n",
    "- *Backpropegation* can be defined by a computational graph:\n",
    "    - This amounts to visiting each edge in a FF network, computing the partial derivative of the next node with respect to the current node; and dot producting that result to the partial derivatives leading into the current node to extend those partial derivatives to the previous nodes (by the *chain rule*)\n",
    "    - since many expressions depend on the same evaluations, table filling -- called *dynamic programming* -- is used, which will lead to a $O(n^2)$ rather than exponential computational demands \n",
    "        - and, actually, given that the usual architecture in a FF network is \"chain structure\", the computational demand with this appraoch is frequently even less than $O(n^2)$.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From your reading, you should know/be aware of the following:\n",
    "\n",
    "- The `xor` was used to show that (single layer) perceptrons had limited representational capacity\n",
    "\n",
    "- Deep networks with only *linear activation functions* can be reduced to a single layer network representing a linear model\n",
    "    - thus we must use \"an affine transformation controlled by learned parameters, followed by a ﬁxed nonlinear function called an activation function\"\n",
    "\n",
    "- Using *calculus of variations*, cost functions can be interpreted as *functionals* (mapping functions to real values)\n",
    "    - so $f(\\textbf{x})$ estimating $E[y|\\textbf{x}]$ is found using mean squared error\n",
    "    - and $f(\\textbf{x})$ estimating $median[y|\\textbf{x}]$ is found using mean absolute error\n",
    "    \n",
    "- We will later see alternative architectures beyond just chained FF networks (at various depths)\n",
    "    - E.g., CNNs to share parameters, and skip connections to improve gradient flow\n",
    "    - Parameter reduction though clever re-use can be an important \"architecture consideration\"\n",
    "        - e.g., as is done with CNNs and can be done in some inistances with (learned) maxout activation functions\n",
    "\n",
    "- Backprop is just an algorithm (with recipes given in 6.5.3/4/6/7) that can be computed \n",
    "    - in $O(n^2)$ time for $n$ the number of edges in a FF network (and constant time derivatives evaluation)\n",
    "    - or better (e.g., in settings such as 6.57) when algebraic/mathematical simplifications can be made.\n",
    "    - This is because it's a general *automatic differentiation* algorithm (a special case of the *reverse mode accumulation* class of algorithms)    \n",
    "    \n",
    "- As noted in Section \"6.5.8 Complications\", there are *many* practical considerations in play when it comes to actual computational implementations of backprop algorithms.  \n",
    "\n",
    "- Backprop is not the only way to go about computing gradients; but, is generally sufficiently effective within the deep learning community so as to make it the \"tool of choice\" with a very dominant \"market share\" in the deep learning context.\n",
    "\n",
    "- There are some things called *Krylov methods* which allow for computationally feasible approximiate computations involving the Hessian: those are called *Krylov methods*\n",
    "\n",
    "- The Historical context of FF networks and MLP as narrated in Section 6.6, particularly the takeaway message that FF networks and MLP are the current state of the art solution of the culmination of efforts to produce general and effective function estimation methodology (and are so effective that they are becoming integral tools enabling *other* tools as foundational underlying subcomponents of those methodologies)\n",
    "\n",
    "- FF networks are function estimation methodologies (Section 6.4.1 Universal Approximation Properties and Depth)\n",
    "    - in terms of function representation, there are results that show expoenential power in representation as a result of depth. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
